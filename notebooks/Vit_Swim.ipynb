{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f41c2136",
   "metadata": {},
   "source": [
    "## testing for Transform Vit Swin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22afc5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 31\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) Setup & configuration\n",
    "# =========================\n",
    "# pip install -U timm torch torchvision torchmetrics\n",
    "\n",
    "import os, math, random, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "\n",
    "# ---- Paths (edit to your dataset root) ----\n",
    "PROC_DIR = Path(r\"C:\\Users\\jians\\Documents\\GitHub\\SMART-Barcode-Scanner-and-Face-Recognition\\data\\data_proc\")\n",
    "TRAIN_DIR = PROC_DIR/\"train\"\n",
    "VAL_DIR   = PROC_DIR/\"val\"\n",
    "TEST_DIR  = PROC_DIR/\"test\"\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# ---- Device (CPU-only as requested) ----\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# ---- Hyperparams (CPU-friendly) ----\n",
    "IMSIZE   = 224          # internal resize\n",
    "BATCH    = 32\n",
    "EPOCHS   = 10           # you can bump to 25 if time allows\n",
    "BASE_LR  = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# ---- Data transforms ----\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((IMSIZE, IMSIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize((IMSIZE, IMSIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "# ---- Datasets & loaders ----\n",
    "train_set = datasets.ImageFolder(TRAIN_DIR, transform=train_tf)\n",
    "val_set   = datasets.ImageFolder(VAL_DIR,   transform=eval_tf)\n",
    "test_set  = datasets.ImageFolder(TEST_DIR,  transform=eval_tf)\n",
    "\n",
    "NUM_CLASSES = len(train_set.classes)\n",
    "print(\"Classes:\", NUM_CLASSES)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH, shuffle=True,  num_workers=2, pin_memory=False)\n",
    "val_loader   = DataLoader(val_set,   batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=False)\n",
    "test_loader  = DataLoader(test_set,  batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0094b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained Swin-Tiny.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) Model: Swin-T backbone\n",
    "#    + 256-D embedding head\n",
    "#    + ArcFace margin head\n",
    "# =========================\n",
    "\n",
    "class L2Norm(nn.Module):\n",
    "    def __init__(self, eps=1e-6): \n",
    "        super().__init__(); self.eps = eps\n",
    "    def forward(self, x): \n",
    "        return F.normalize(x, dim=1, eps=self.eps)\n",
    "\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    \"\"\"ArcFace head: additive angular margin.\"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, emb, labels):\n",
    "        # emb: (B, D) L2-normalized\n",
    "        # labels: (B,) long\n",
    "        W = F.normalize(self.weight)\n",
    "        cosine = F.linear(emb, W)  # (B, C)\n",
    "        sine = torch.sqrt(torch.clamp(1.0 - cosine**2, min=1e-9))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, labels.view(-1,1), 1.0)\n",
    "        logits = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        logits *= self.s\n",
    "        return logits  # (B, C)\n",
    "\n",
    "class SwinTinyEmbedder(nn.Module):\n",
    "    def __init__(self, embed_dim=256, pretrained=True):\n",
    "        super().__init__()\n",
    "        # Try to load pretrained; fall back gracefully\n",
    "        try:\n",
    "            self.backbone = timm.create_model(\n",
    "                \"swin_tiny_patch4_window7_224\",\n",
    "                pretrained=pretrained,\n",
    "                num_classes=0,   # feature extractor\n",
    "                global_pool=\"avg\"\n",
    "            )\n",
    "            print(\"Loaded pretrained Swin-Tiny.\")\n",
    "        except Exception as e:\n",
    "            print(\"Could not load pretrained weights, using random init:\", e)\n",
    "            self.backbone = timm.create_model(\n",
    "                \"swin_tiny_patch4_window7_224\",\n",
    "                pretrained=False,\n",
    "                num_classes=0,\n",
    "                global_pool=\"avg\"\n",
    "            )\n",
    "        in_feats = self.backbone.num_features\n",
    "        self.proj = nn.Linear(in_feats, embed_dim, bias=False)\n",
    "        self.l2 = L2Norm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)     # (B, F)\n",
    "        e = self.proj(f)         # (B, embed_dim)\n",
    "        e = self.l2(e)           # L2-normalized\n",
    "        return e\n",
    "\n",
    "embed_dim = 256\n",
    "embedder = SwinTinyEmbedder(embed_dim=embed_dim, pretrained=True).to(device)\n",
    "arcface  = ArcMarginProduct(in_features=embed_dim, out_features=NUM_CLASSES, s=30.0, m=0.5).to(device)\n",
    "\n",
    "# Simple wrapper for training\n",
    "class FRModel(nn.Module):\n",
    "    def __init__(self, embedder, arcface):\n",
    "        super().__init__()\n",
    "        self.embedder = embedder\n",
    "        self.arcface = arcface\n",
    "    def forward(self, x, labels=None):\n",
    "        emb = self.embedder(x)\n",
    "        if labels is None:\n",
    "            return emb\n",
    "        logits = self.arcface(emb, labels)\n",
    "        return logits, emb\n",
    "\n",
    "model = FRModel(embedder, arcface).to(device)\n",
    "\n",
    "# Optimizer & schedule\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "# Cosine decay with warmup\n",
    "total_steps = max(len(train_loader)*EPOCHS, 1)\n",
    "warmup_steps = int(0.05 * total_steps)\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / max(1, warmup_steps)\n",
    "    p = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * p))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1431726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Checkpoint utils =====\n",
    "import os, torch\n",
    "\n",
    "CKPT_DIR = \"checkpoints_transformer\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "def _model_state(m):\n",
    "    # handles DataParallel/DistributedDataParallel transparently\n",
    "    return m.module.state_dict() if hasattr(m, \"module\") else m.state_dict()\n",
    "\n",
    "def save_checkpoint(path, epoch, model, optimizer=None, scheduler=None, best_val_acc=None, extra=None):\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": _model_state(model),\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "    }\n",
    "    if optimizer is not None:\n",
    "        ckpt[\"optimizer_state_dict\"] = optimizer.state_dict()\n",
    "    if scheduler is not None:\n",
    "        ckpt[\"scheduler_state_dict\"] = scheduler.state_dict()\n",
    "    if extra is not None:\n",
    "        ckpt[\"extra\"] = extra\n",
    "    torch.save(ckpt, path)\n",
    "\n",
    "def load_checkpoint(path, model, optimizer=None, scheduler=None, map_location=\"cpu\"):\n",
    "    ckpt = torch.load(path, map_location=map_location)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    if optimizer is not None and \"optimizer_state_dict\" in ckpt:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "    if scheduler is not None and \"scheduler_state_dict\" in ckpt:\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "    start_epoch = ckpt.get(\"epoch\", 0) + 1\n",
    "    best_val_acc = ckpt.get(\"best_val_acc\", 0.0)\n",
    "    return start_epoch, best_val_acc, ckpt.get(\"extra\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0dfa7a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/10 | train_acc 0.016 loss 15.908 | val_acc 0.073 loss 13.887 | 369.9s\n",
      "✅ New best @ epoch 1: val_acc=0.0728 (saved)\n",
      "Epoch 02/10 | train_acc 0.173 loss 11.191 | val_acc 0.328 loss 8.502 | 262.1s\n",
      "✅ New best @ epoch 2: val_acc=0.3277 (saved)\n",
      "Epoch 03/10 | train_acc 0.467 loss 5.837 | val_acc 0.557 loss 5.647 | 317.3s\n",
      "✅ New best @ epoch 3: val_acc=0.5574 (saved)\n",
      "Epoch 04/10 | train_acc 0.651 loss 3.542 | val_acc 0.588 loss 5.839 | 254.0s\n",
      "✅ New best @ epoch 4: val_acc=0.5882 (saved)\n",
      "Epoch 05/10 | train_acc 0.804 loss 1.596 | val_acc 0.745 loss 3.903 | 278.4s\n",
      "✅ New best @ epoch 5: val_acc=0.7451 (saved)\n",
      "Epoch 06/10 | train_acc 0.906 loss 0.689 | val_acc 0.804 loss 3.062 | 301.0s\n",
      "✅ New best @ epoch 6: val_acc=0.8039 (saved)\n",
      "Epoch 07/10 | train_acc 0.955 loss 0.226 | val_acc 0.824 loss 2.687 | 308.8s\n",
      "✅ New best @ epoch 7: val_acc=0.8235 (saved)\n",
      "Epoch 08/10 | train_acc 0.972 loss 0.150 | val_acc 0.840 loss 2.677 | 310.2s\n",
      "✅ New best @ epoch 8: val_acc=0.8403 (saved)\n",
      "Epoch 09/10 | train_acc 0.974 loss 0.145 | val_acc 0.840 loss 2.547 | 309.0s\n",
      "Epoch 10/10 | train_acc 0.979 loss 0.089 | val_acc 0.840 loss 2.541 | 310.7s\n",
      "Training done. Best val_acc=0.8403 @ epoch 8\n",
      "Loaded best checkpoint for evaluation.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2) Train (classification)\n",
    "# =========================\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "    running_loss, correct, count = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(x, y)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # <- you were stepping per-batch; keep if intended\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                logits, _ = model(x, y)\n",
    "                loss = criterion(logits, y)\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        count += x.size(0)\n",
    "    return running_loss / max(1, count), correct / max(1, count)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
    "    val_loss, val_acc = run_epoch(val_loader,   train=False)\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "          f\"train_acc {tr_acc:.3f} loss {tr_loss:.3f} | \"\n",
    "          f\"val_acc {val_acc:.3f} loss {val_loss:.3f} | {dt:.1f}s\")\n",
    "\n",
    "    # ---- Save \"last\" checkpoint every epoch ----\n",
    "    save_checkpoint(\n",
    "        path=os.path.join(CKPT_DIR, \"last.pt\"),\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        best_val_acc=best_val_acc,\n",
    "        extra={\"val_acc\": float(val_acc), \"val_loss\": float(val_loss)}\n",
    "    )\n",
    "    # Also keep a light weights-only file\n",
    "    torch.save(_model_state(model), os.path.join(CKPT_DIR, \"last_weights.pth\"))\n",
    "\n",
    "    # ---- Save \"best\" when validation improves ----\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        save_checkpoint(\n",
    "            path=os.path.join(CKPT_DIR, \"best.pt\"),\n",
    "            epoch=epoch,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            best_val_acc=best_val_acc,\n",
    "            extra={\"val_acc\": float(val_acc), \"val_loss\": float(val_loss)}\n",
    "        )\n",
    "        torch.save(_model_state(model), os.path.join(CKPT_DIR, \"best_weights.pth\"))\n",
    "        print(f\"✅ New best @ epoch {epoch}: val_acc={val_acc:.4f} (saved)\")\n",
    "\n",
    "print(f\"Training done. Best val_acc={best_val_acc:.4f} @ epoch {best_epoch}\")\n",
    "\n",
    "# Optionally load the best weights into model for downstream eval/inference:\n",
    "start_epoch, best_val_acc_loaded, _ = load_checkpoint(\n",
    "    os.path.join(CKPT_DIR, \"best.pt\"), model, optimizer=None, scheduler=None, map_location=device\n",
    ")\n",
    "print(\"Loaded best checkpoint for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3abd8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Metrics (ViT) ===\n",
      "Loss        : 2.2504\n",
      "Top-1 Acc   : 0.8375\n",
      "Top-5 Acc : 0.9175\n",
      "Precision   : macro=0.8404 | micro=0.8375 | weighted=0.8432\n",
      "Recall      : macro=0.8293 | micro=0.8375 | weighted=0.8375\n",
      "F1-score    : macro=0.8311  | micro=0.8375  | weighted=0.8366\n",
      "ROC-AUC(m)  : 0.9316058509960582\n",
      "\n",
      "Per-class accuracy:\n",
      " - Akshay Kumar: 1.0000\n",
      " - Alexandra Daddario: 0.9333\n",
      " - Alia Bhatt: 0.7500\n",
      " - Amitabh Bachchan: 0.8333\n",
      " - Andy Samberg: 0.8667\n",
      " - Anushka Sharma: 0.6364\n",
      " - Billie Eilish: 0.6429\n",
      " - Brad Pitt: 0.8947\n",
      " - Camila Cabello: 0.9231\n",
      " - Charlize Theron: 0.8462\n",
      " - Claire Holt: 0.6667\n",
      " - Courtney Cox: 0.7500\n",
      " - Dwayne Johnson: 1.0000\n",
      " - Elizabeth Olsen: 0.8182\n",
      " - Ellen Degeneres: 0.9167\n",
      " - Henry Cavill: 0.9412\n",
      " - Hrithik Roshan: 0.7333\n",
      " - Hugh Jackman: 0.7647\n",
      " - Jessica Alba: 0.7647\n",
      " - Kashyap: 0.6000\n",
      " - Lisa Kudrow: 0.9091\n",
      " - Margot Robbie: 0.8333\n",
      " - Marmik: 0.6000\n",
      " - Natalie Portman: 0.9412\n",
      " - Priyanka Chopra: 1.0000\n",
      " - Robert Downey Jr: 0.8889\n",
      " - Roger Federer: 0.7273\n",
      " - Tom Cruise: 1.0000\n",
      " - Vijay Deverakonda: 0.7647\n",
      " - Virat Kohli: 0.8333\n",
      " - Zac Efron: 0.9286\n",
      "\n",
      "Classification report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "      Akshay Kumar     1.0000    1.0000    1.0000         8\n",
      "Alexandra Daddario     0.9333    0.9333    0.9333        15\n",
      "        Alia Bhatt     0.7500    0.7500    0.7500        12\n",
      "  Amitabh Bachchan     1.0000    0.8333    0.9091        12\n",
      "      Andy Samberg     0.8667    0.8667    0.8667        15\n",
      "    Anushka Sharma     0.8750    0.6364    0.7368        11\n",
      "     Billie Eilish     0.8182    0.6429    0.7200        14\n",
      "         Brad Pitt     0.8095    0.8947    0.8500        19\n",
      "    Camila Cabello     0.9231    0.9231    0.9231        13\n",
      "   Charlize Theron     0.6111    0.8462    0.7097        13\n",
      "       Claire Holt     0.6667    0.6667    0.6667        15\n",
      "      Courtney Cox     0.9000    0.7500    0.8182        12\n",
      "    Dwayne Johnson     0.9091    1.0000    0.9524        10\n",
      "   Elizabeth Olsen     0.9000    0.8182    0.8571        11\n",
      "   Ellen Degeneres     1.0000    0.9167    0.9565        12\n",
      "      Henry Cavill     0.9412    0.9412    0.9412        17\n",
      "    Hrithik Roshan     0.8462    0.7333    0.7857        15\n",
      "      Hugh Jackman     0.7647    0.7647    0.7647        17\n",
      "      Jessica Alba     0.8667    0.7647    0.8125        17\n",
      "           Kashyap     0.7500    0.6000    0.6667         5\n",
      "       Lisa Kudrow     0.9091    0.9091    0.9091        11\n",
      "     Margot Robbie     0.8333    0.8333    0.8333        12\n",
      "            Marmik     0.6000    0.6000    0.6000         5\n",
      "   Natalie Portman     0.8889    0.9412    0.9143        17\n",
      "   Priyanka Chopra     0.7619    1.0000    0.8649        16\n",
      "  Robert Downey Jr     0.8000    0.8889    0.8421        18\n",
      "     Roger Federer     0.7273    0.7273    0.7273        11\n",
      "        Tom Cruise     0.8333    1.0000    0.9091        10\n",
      " Vijay Deverakonda     0.8667    0.7647    0.8125        17\n",
      "       Virat Kohli     0.8333    0.8333    0.8333         6\n",
      "         Zac Efron     0.8667    0.9286    0.8966        14\n",
      "\n",
      "          accuracy                         0.8375       400\n",
      "         macro avg     0.8404    0.8293    0.8311       400\n",
      "      weighted avg     0.8432    0.8375    0.8366       400\n",
      "\n",
      "\n",
      "Saved: artifacts/vit_test_metrics.json and artifacts/vit_confusion_matrix.npy\n"
     ]
    }
   ],
   "source": [
    "# === Evaluation: load best.pt and compute metrics on test_loader ===\n",
    "import os, json, numpy as np, torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    classification_report, confusion_matrix, roc_auc_score\n",
    ")\n",
    "\n",
    "CKPT_DIR = \"checkpoints_transformer\"\n",
    "OUT_DIR  = \"artifacts\"\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Load best checkpoint into the existing model\n",
    "_ = load_checkpoint(os.path.join(CKPT_DIR, \"best.pt\"), model, optimizer=None, scheduler=None, map_location=device)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 2) Run through test set and collect logits/probs/labels\n",
    "y_true, y_pred, y_prob_list = [], [], []\n",
    "running_loss, n_samples = 0.0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        out = model(xb, yb)  # your forward accepts (x, y); adjust to model(xb) if not needed\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "        loss = criterion(logits, yb)\n",
    "        running_loss += float(loss.item()) * xb.size(0)\n",
    "        n_samples += xb.size(0)\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        y_prob_list.append(probs.detach().cpu().numpy())\n",
    "        y_true.append(yb.detach().cpu().numpy())\n",
    "        y_pred.append(preds.detach().cpu().numpy())\n",
    "\n",
    "test_loss = running_loss / max(1, n_samples)\n",
    "y_prob = np.concatenate(y_prob_list, axis=0)\n",
    "y_true = np.concatenate(y_true, axis=0).astype(int)\n",
    "y_pred = np.concatenate(y_pred, axis=0).astype(int)\n",
    "num_classes = y_prob.shape[1]\n",
    "\n",
    "# Try to get class names from the dataset; else 0..C-1\n",
    "try:\n",
    "    CLASS_NAMES = list(getattr(test_loader.dataset, \"classes\"))\n",
    "    if len(CLASS_NAMES) != num_classes:\n",
    "        CLASS_NAMES = [str(i) for i in range(num_classes)]\n",
    "except Exception:\n",
    "    CLASS_NAMES = [str(i) for i in range(num_classes)]\n",
    "\n",
    "# 3) Metrics\n",
    "top1_acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "k = min(5, num_classes)\n",
    "if k > 1:\n",
    "    topk_idx = np.argsort(y_prob, axis=1)[:, -k:]\n",
    "    topk_acc = np.mean([y_true[i] in topk_idx[i] for i in range(len(y_true))])\n",
    "else:\n",
    "    topk_acc = top1_acc\n",
    "\n",
    "prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=\"macro\", zero_division=0\n",
    ")\n",
    "prec_micro, rec_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=\"micro\", zero_division=0\n",
    ")\n",
    "prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=\"weighted\", zero_division=0\n",
    ")\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "per_class_acc = {}\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    row = cm[i].sum()\n",
    "    per_class_acc[name] = float(cm[i, i] / row) if row > 0 else 0.0\n",
    "\n",
    "# ROC-AUC (OvR macro). Needs one-hot labels\n",
    "try:\n",
    "    y_true_1h = np.eye(num_classes, dtype=np.float32)[y_true]\n",
    "    roc_auc_macro = float(roc_auc_score(y_true_1h, y_prob, multi_class=\"ovr\", average=\"macro\"))\n",
    "except Exception:\n",
    "    roc_auc_macro = None\n",
    "\n",
    "# 4) Print summary\n",
    "print(\"\\n=== Test Metrics (ViT) ===\")\n",
    "print(f\"Loss        : {test_loss:.4f}\")\n",
    "print(f\"Top-1 Acc   : {top1_acc:.4f}\")\n",
    "print(f\"Top-{k} Acc : {topk_acc:.4f}\")\n",
    "print(f\"Precision   : macro={prec_macro:.4f} | micro={prec_micro:.4f} | weighted={prec_weighted:.4f}\")\n",
    "print(f\"Recall      : macro={rec_macro:.4f} | micro={rec_micro:.4f} | weighted={rec_weighted:.4f}\")\n",
    "print(f\"F1-score    : macro={f1_macro:.4f}  | micro={f1_micro:.4f}  | weighted={f1_weighted:.4f}\")\n",
    "print(f\"ROC-AUC(m)  : {roc_auc_macro if roc_auc_macro is not None else 'N/A'}\")\n",
    "\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for name, acc in per_class_acc.items():\n",
    "    print(f\" - {name}: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4, zero_division=0))\n",
    "\n",
    "# 5) Save artifacts\n",
    "report = {\n",
    "    \"loss\": float(test_loss),\n",
    "    \"top1_acc\": float(top1_acc),\n",
    "    f\"top{k}_acc\": float(topk_acc),\n",
    "    \"precision\": {\"macro\": float(prec_macro), \"micro\": float(rec_micro), \"weighted\": float(prec_weighted)},\n",
    "    \"recall\":    {\"macro\": float(rec_macro), \"micro\": float(rec_micro), \"weighted\": float(rec_weighted)},\n",
    "    \"f1\":        {\"macro\": float(f1_macro),  \"micro\": float(f1_micro),  \"weighted\": float(f1_weighted)},\n",
    "    \"roc_auc_macro_ovr\": roc_auc_macro,\n",
    "    \"per_class_accuracy\": per_class_acc,\n",
    "    \"class_names\": CLASS_NAMES,\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"vit_test_metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "np.save(os.path.join(OUT_DIR, \"vit_confusion_matrix.npy\"), cm)\n",
    "print(f\"\\nSaved: {OUT_DIR}/vit_test_metrics.json and {OUT_DIR}/vit_confusion_matrix.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f25e8652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cpu\n",
      "[INFO] IMSIZE=224 | EMBED_DIM=256 | mean=[0.5, 0.5, 0.5] | std=[0.5, 0.5, 0.5]\n",
      "[INFO] Threshold tau=0.850\n",
      "Press SPACE to capture, ESC to quit.\n",
      "✅ Captured embedding shape: (256,)\n",
      "Press SPACE to capture, ESC to quit.\n",
      "✅ Captured embedding shape: (256,)\n",
      "\n",
      "cosine=0.892  |  tau=0.850  ->  MATCH ✅\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Webcam -> two captures -> verify (SwinTiny)\n",
    "# ============================================\n",
    "# pip install opencv-python timm torch torchvision pillow\n",
    "\n",
    "import os, sys, math, time, numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2, timm\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "CKPT_PATH     = r\"C:\\Users\\jians\\Documents\\GitHub\\SMART-Barcode-Scanner-and-Face-Recognition\\checkpoints_transformer\\best.pt\"   # your saved checkpoint\n",
    "TEMPLATES_NPZ = \"swin_templates.npz\"      # optional (load tau)\n",
    "CAM_INDEX     = 0                         # webcam index\n",
    "EXPAND_X      = 0.15                      # face box horizontal expansion\n",
    "EXPAND_Y      = 0.20                      # face box vertical expansion\n",
    "MIN_FACE      = (60, 60)                  # min face size in pixels\n",
    "DEFAULT_TAU   = 0.85                      # fallback threshold\n",
    "# ----------------------------------------\n",
    "\n",
    "# ------------- Device setup -------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "print(\"[INFO] Device:\", device)\n",
    "\n",
    "# ------------- Model (embedder) ----------\n",
    "class L2Norm(nn.Module):\n",
    "    def __init__(self, eps=1e-6): \n",
    "        super().__init__(); self.eps = eps\n",
    "    def forward(self, x): \n",
    "        return F.normalize(x, dim=1, eps=self.eps)\n",
    "\n",
    "class SwinTinyEmbedder(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            \"swin_tiny_patch4_window7_224\",\n",
    "            pretrained=False, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        in_feats = self.backbone.num_features\n",
    "        self.proj = nn.Linear(in_feats, embed_dim, bias=False)\n",
    "        self.l2 = L2Norm()\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        e = self.proj(f)\n",
    "        return self.l2(e)\n",
    "\n",
    "# --------- Load checkpoint & config ---------\n",
    "if not os.path.exists(CKPT_PATH):\n",
    "    print(f\"[ERR] Missing checkpoint: {CKPT_PATH}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "\n",
    "# Try to get training-time preprocessing from ckpt; set sensible defaults otherwise\n",
    "IMSIZE = int(ckpt.get(\"imsize\", 224))  # default 224 (Swin)\n",
    "MEAN   = ckpt.get(\"mean\", [0.5, 0.5, 0.5])\n",
    "STD    = ckpt.get(\"std\",  [0.5, 0.5, 0.5])\n",
    "EMBED_DIM = int(ckpt.get(\"embed_dim\", 256))\n",
    "\n",
    "print(f\"[INFO] IMSIZE={IMSIZE} | EMBED_DIM={EMBED_DIM} | mean={MEAN} | std={STD}\")\n",
    "\n",
    "embedder = SwinTinyEmbedder(embed_dim=EMBED_DIM).to(device)\n",
    "\n",
    "# Robustly extract an embedder state dict from various checkpoint formats\n",
    "state = None\n",
    "candidates = [\n",
    "    \"embedder_state\", \"model_state_dict\", \"state_dict\", \"model_state\"\n",
    "]\n",
    "for k in candidates:\n",
    "    if k in ckpt and isinstance(ckpt[k], dict):\n",
    "        state = ckpt[k]\n",
    "        break\n",
    "if state is None and isinstance(ckpt, dict):\n",
    "    # If top-level are params already\n",
    "    if all(isinstance(v, torch.Tensor) for v in ckpt.values()):\n",
    "        state = ckpt\n",
    "\n",
    "if state is None:\n",
    "    print(\"[ERR] Could not find a state_dict in checkpoint.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# If keys are prefixed like \"embedder.backbone....\", strip \"embedder.\"\n",
    "def strip_prefix(d, prefix=\"embedder.\"):\n",
    "    return { (k[len(prefix):] if k.startswith(prefix) else k): v for k,v in d.items() }\n",
    "\n",
    "state = strip_prefix(state, \"embedder.\")\n",
    "# Filter to only keys that exist in our embedder (prevents unexpected key errors)\n",
    "expected = embedder.state_dict().keys()\n",
    "filtered = {k: v for k, v in state.items() if k in expected}\n",
    "\n",
    "missing, unexpected = embedder.load_state_dict(filtered, strict=False)\n",
    "if missing:   print(\"[WARN] Missing keys:\", missing[:5], \"...\" if len(missing)>5 else \"\")\n",
    "if unexpected:print(\"[WARN] Unexpected keys (ignored):\", unexpected[:5], \"...\" if len(unexpected)>5 else \"\")\n",
    "embedder.eval()\n",
    "\n",
    "# ------------- Threshold (tau) -------------\n",
    "TAU = DEFAULT_TAU\n",
    "if os.path.exists(TEMPLATES_NPZ):\n",
    "    try:\n",
    "        npz = np.load(TEMPLATES_NPZ, allow_pickle=True)\n",
    "        if \"tau\" in npz.files:\n",
    "            val = np.array(npz[\"tau\"]).reshape(-1)[0]\n",
    "            TAU = float(val)\n",
    "            print(f\"[INFO] Using tau from {TEMPLATES_NPZ}: {TAU:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Could not load tau from templates:\", e)\n",
    "print(f\"[INFO] Threshold tau={TAU:.3f}\")\n",
    "\n",
    "# ------------- Preprocessing --------------\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize((IMSIZE, IMSIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_np_image_bgr(bgr):\n",
    "    \"\"\"\n",
    "    bgr: (H,W,3) uint8 face crop\n",
    "    returns: (D,) float32 L2-normalized embedding on CPU\n",
    "    \"\"\"\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(rgb)\n",
    "    x = eval_tf(img).unsqueeze(0).to(device)      # (1,3,H,W)\n",
    "    e = embedder(x).detach().cpu().numpy()[0]\n",
    "    return e.astype(np.float32)\n",
    "\n",
    "def cosine(a, b):  # embeddings are L2 -> dot == cosine\n",
    "    return float(np.dot(a, b))\n",
    "\n",
    "# ----------- Face detector utils ----------\n",
    "def get_face_cascade():\n",
    "    candidates = [\n",
    "        \"models/haarcascade_frontalface_default.xml\",\n",
    "        os.path.join(os.getcwd(), \"haarcascade_frontalface_default.xml\"),\n",
    "        os.path.join(cv2.data.haarcascades, \"haarcascade_frontalface_default.xml\"),\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            cc = cv2.CascadeClassifier(p)\n",
    "            if not cc.empty():\n",
    "                return cc\n",
    "    raise IOError(\"Haar cascade not found. Put 'haarcascade_frontalface_default.xml' in ./models/ or project root.\")\n",
    "\n",
    "_FACE = get_face_cascade()\n",
    "\n",
    "def detect_largest_face(gray):\n",
    "    faces = _FACE.detectMultiScale(\n",
    "        gray, scaleFactor=1.1, minNeighbors=4,\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE, minSize=MIN_FACE\n",
    "    )\n",
    "    if len(faces) == 0: return None\n",
    "    return max(faces, key=lambda r: r[2]*r[3])  # (x,y,w,h)\n",
    "\n",
    "def expand_box(x, y, w, h, fx=EXPAND_X, fy=EXPAND_Y, W=0, H=0):\n",
    "    cx, cy = x + w//2, y + h//2\n",
    "    w2, h2 = int(w*(1+2*fx)), int(h*(1+2*fy))\n",
    "    x2, y2 = max(0, cx - w2//2), max(0, cy - h2//2)\n",
    "    x2e, y2e = min(W, x2 + w2), min(H, y2 + h2)\n",
    "    return x2, y2, max(1, x2e-x2), max(1, y2e-y2)\n",
    "\n",
    "# -------- Capture one face & embed --------\n",
    "def capture_and_embed(cam_index=CAM_INDEX, window_name=\"Camera\"):\n",
    "    cap = cv2.VideoCapture(cam_index, cv2.CAP_DSHOW)\n",
    "    if not cap.isOpened():\n",
    "        print(\"[ERR] Cannot open camera\")\n",
    "        return None\n",
    "    print(\"Press SPACE to capture, ESC to quit.\")\n",
    "    emb = None\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            print(\"[ERR] Frame grab failed\"); break\n",
    "        frame = cv2.flip(frame, 1)  # mirror for user\n",
    "        gray  = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        box = detect_largest_face(gray)\n",
    "        if box is not None:\n",
    "            x,y,w,h = box\n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "            cv2.putText(frame, \"Face\", (x, y-8), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(window_name, frame)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:  # ESC\n",
    "            print(\"Exit.\"); break\n",
    "        if k == 32:  # SPACE\n",
    "            if box is None:\n",
    "                print(\"❌ No face detected. Try again.\")\n",
    "                continue\n",
    "            H, W = frame.shape[:2]\n",
    "            x,y,w,h = expand_box(*box, W=W, H=H)\n",
    "            face = frame[y:y+h, x:x+w].copy()\n",
    "            emb = embed_np_image_bgr(face)\n",
    "            print(\"✅ Captured embedding shape:\", emb.shape)\n",
    "            break\n",
    "    cap.release(); cv2.destroyAllWindows()\n",
    "    return emb\n",
    "\n",
    "# ---------- Optional: enroll & verify helpers ----------\n",
    "def enroll_and_save(name=\"user1\", out_dir=\"enrollments\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    e = capture_and_embed()\n",
    "    if e is None: \n",
    "        print(\"❌ Enrollment failed.\")\n",
    "        return None\n",
    "    path = os.path.join(out_dir, f\"{name}.npy\")\n",
    "    np.save(path, e.astype(np.float32))\n",
    "    print(f\"✔ Saved: {path}\")\n",
    "    return path\n",
    "\n",
    "def verify_against_file(path, tau=TAU):\n",
    "    ref = np.load(path)\n",
    "    probe = capture_and_embed()\n",
    "    if probe is None:\n",
    "        print(\"❌ Probe capture failed.\")\n",
    "        return None, None\n",
    "    sim = cosine(ref, probe)\n",
    "    ok = sim >= tau\n",
    "    print(f\"[VERIFY] cosine={sim:.4f} | tau={tau:.2f} → {'MATCH ✅' if ok else 'NOT MATCH ❌'}\")\n",
    "    return sim, ok\n",
    "\n",
    "# -------------- Main: 2-capture verify --------------\n",
    "if __name__ == \"__main__\":\n",
    "    e1 = capture_and_embed()\n",
    "    if e1 is None: sys.exit(0)\n",
    "    e2 = capture_and_embed()\n",
    "    if e2 is None: sys.exit(0)\n",
    "\n",
    "    sim = cosine(e1, e2)\n",
    "    verdict = \"MATCH ✅\" if sim >= TAU else \"NOT MATCH ❌\"\n",
    "    print(f\"\\ncosine={sim:.3f}  |  tau={TAU:.3f}  ->  {verdict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61146790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cpu\n",
      "[INFO] Using checkpoint: C:\\Users\\jians\\Documents\\GitHub\\SMART-Barcode-Scanner-and-Face-Recognition\\checkpoints_transformer\\best.pt\n",
      "[INFO] IMSIZE=224 | EMBED_DIM=256 | mean/std=[0.5, 0.5, 0.5]/[0.5, 0.5, 0.5]\n",
      "[INFO] Embedder ready, params loaded.\n",
      "[INFO] Threshold tau=0.850\n",
      "[INFO] Starting two-capture verify (Haar + averaging + quality gate).\n",
      "Press SPACE to capture 5 frames, ESC to quit.\n",
      "Press SPACE to capture 5 frames, ESC to quit.\n",
      "[VERIFY] cosine=0.7772 | tau=0.85 → NOT MATCH ❌\n",
      "\n",
      "Result → cosine=0.7772 vs tau=0.85 :: NOT MATCH ❌\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Webcam -> robust capture -> verify (SwinTiny)\n",
    "# No MediaPipe; Haar only; multi-frame averaging + quality gate\n",
    "# ============================================\n",
    "# pip install opencv-python timm torch torchvision pillow\n",
    "\n",
    "import os, sys, time, math, numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2, timm\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "# Put your absolute or relative checkpoint path here:\n",
    "# Examples:\n",
    "#   CKPT_PATH = r\"C:\\Users\\you\\model\\best.pth\"\n",
    "#   CKPT_PATH = \"checkpoints/swin_arcface_best.pth\"\n",
    "CKPT_PATH    = r\"C:\\Users\\jians\\Documents\\GitHub\\SMART-Barcode-Scanner-and-Face-Recognition\\checkpoints_transformer\\best.pt\"  # <-- change this\n",
    "CAM_INDEX    = 0\n",
    "DEFAULT_TAU  = 0.85\n",
    "\n",
    "# Face crop & quality\n",
    "IMSIZE_FALLBK = 224\n",
    "EMB_FALLBK    = 256\n",
    "EXPAND_X      = 0.20     # expand Haar box (left/right)\n",
    "EXPAND_Y      = 0.30     # expand Haar box (up/down, with more forehead)\n",
    "MIN_FACE_PX   = 80       # reject tiny faces\n",
    "MIN_BLUR      = 60.0     # Laplacian var threshold\n",
    "MIN_BRIGHT    = 35       # mean gray min\n",
    "MAX_BRIGHT    = 230      # mean gray max\n",
    "ENROLL_AVG_N  = 5        # frames to average for enrollment\n",
    "PROBE_AVG_N   = 5        # frames to average for probe\n",
    "\n",
    "# ------------- Device setup -------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "print(\"[INFO] Device:\", device)\n",
    "\n",
    "# ------------- Path resolve -------------\n",
    "def resolve_ckpt(path_or_base):\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      - exact file (absolute or relative): *.pth or *.pt\n",
    "      - base name without ext: will try .pth then .pt\n",
    "    Returns a normalized absolute path or raises FileNotFoundError.\n",
    "    \"\"\"\n",
    "    p = Path(os.path.expanduser(str(path_or_base))).expanduser()\n",
    "    # exact file\n",
    "    if p.suffix.lower() in (\".pth\", \".pt\"):\n",
    "        if p.exists():\n",
    "            return p.resolve()\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Checkpoint not found: {p}\")\n",
    "    # try with common extensions\n",
    "    for ext in (\".pth\", \".pt\"):\n",
    "        cand = (p.parent / (p.name + ext)).resolve()\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    # if a directory was passed, try common filenames inside\n",
    "    if p.exists() and p.is_dir():\n",
    "        for name in (\"best.pth\", \"best.pt\", \"last.pth\", \"last.pt\"):\n",
    "            cand = (p / name).resolve()\n",
    "            if cand.exists():\n",
    "                return cand\n",
    "        # fallback: any .pth/.pt, pick newest\n",
    "        any_cand = list(p.glob(\"*.pth\")) + list(p.glob(\"*.pt\"))\n",
    "        if any_cand:\n",
    "            return max(any_cand, key=lambda q: q.stat().st_mtime).resolve()\n",
    "    raise FileNotFoundError(f\"No checkpoint found for '{path_or_base}'\")\n",
    "\n",
    "# ------------- Model (embedder) ----------\n",
    "class L2Norm(nn.Module):\n",
    "    def __init__(self, eps=1e-6): super().__init__(); self.eps = eps\n",
    "    def forward(self, x): return F.normalize(x, dim=1, eps=self.eps)\n",
    "\n",
    "class SwinTinyEmbedder(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            \"swin_tiny_patch4_window7_224\",\n",
    "            pretrained=False, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        in_feats = self.backbone.num_features\n",
    "        self.proj = nn.Linear(in_feats, embed_dim, bias=False)\n",
    "        self.l2 = L2Norm()\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        e = self.proj(f)\n",
    "        return self.l2(e)\n",
    "\n",
    "# --------- Load checkpoint & config ---------\n",
    "try:\n",
    "    ckpt_path = resolve_ckpt(CKPT_PATH)\n",
    "except FileNotFoundError as e:\n",
    "    print(\"[ERR]\", e)\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"[INFO] Using checkpoint: {ckpt_path}\")\n",
    "ckpt = torch.load(str(ckpt_path), map_location=\"cpu\")\n",
    "\n",
    "# Training-time preprocessing (fall back if missing)\n",
    "IMSIZE    = int(ckpt.get(\"imsize\", IMSIZE_FALLBK))\n",
    "EMBED_DIM = int(ckpt.get(\"embed_dim\", EMB_FALLBK))\n",
    "MEAN      = ckpt.get(\"mean\", [0.5, 0.5, 0.5])\n",
    "STD       = ckpt.get(\"std\",  [0.5, 0.5, 0.5])\n",
    "print(f\"[INFO] IMSIZE={IMSIZE} | EMBED_DIM={EMBED_DIM} | mean/std={MEAN}/{STD}\")\n",
    "\n",
    "embedder = SwinTinyEmbedder(embed_dim=EMBED_DIM).to(device)\n",
    "\n",
    "# Robustly extract a state dict\n",
    "state = None\n",
    "for k in (\"embedder_state\",\"model_state_dict\",\"state_dict\",\"model_state\"):\n",
    "    if k in ckpt and isinstance(ckpt[k], dict):\n",
    "        state = ckpt[k]; break\n",
    "# if directly a state dict\n",
    "if state is None and isinstance(ckpt, dict) and all(isinstance(v, torch.Tensor) for v in ckpt.values()):\n",
    "    state = ckpt\n",
    "\n",
    "if state is None:\n",
    "    print(\"[ERR] Could not find a state_dict in checkpoint.\"); sys.exit(1)\n",
    "\n",
    "# Strip possible prefix \"embedder.\"\n",
    "state = { (k.split(\"embedder.\",1)[-1] if k.startswith(\"embedder.\") else k): v for k,v in state.items() }\n",
    "\n",
    "# Filter to expected keys\n",
    "expected = embedder.state_dict().keys()\n",
    "state = {k:v for k,v in state.items() if k in expected}\n",
    "\n",
    "missing, unexpected = embedder.load_state_dict(state, strict=False)\n",
    "if missing:   print(\"[WARN] Missing keys:\", missing[:6], \"...\" if len(missing)>6 else \"\")\n",
    "if unexpected: print(\"[WARN] Unexpected keys (ignored):\", unexpected[:6], \"...\" if len(unexpected)>6 else \"\")\n",
    "embedder.eval()\n",
    "print(\"[INFO] Embedder ready, params loaded.\")\n",
    "\n",
    "# ------------- Threshold (tau) -------------\n",
    "TAU = DEFAULT_TAU\n",
    "print(f\"[INFO] Threshold tau={TAU:.3f}\")\n",
    "\n",
    "# ------------- Preprocessing --------------\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize((IMSIZE, IMSIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_bgr(face_bgr):\n",
    "    \"\"\"Face BGR -> embedding (L2-normalized)\"\"\"\n",
    "    rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(rgb)\n",
    "    x = eval_tf(img).unsqueeze(0).to(device)\n",
    "    e = embedder(x).detach().cpu().numpy()[0].astype(np.float32)\n",
    "    return e\n",
    "\n",
    "def cosine(a, b):\n",
    "    a = a / (np.linalg.norm(a)+1e-9)\n",
    "    b = b / (np.linalg.norm(b)+1e-9)\n",
    "    return float(np.dot(a, b))\n",
    "\n",
    "# ----------- Haar face detector ----------\n",
    "_HAAR = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def detect_largest(gray):\n",
    "    faces = _HAAR.detectMultiScale(\n",
    "        gray, scaleFactor=1.1, minNeighbors=4,\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE, minSize=(MIN_FACE_PX, MIN_FACE_PX)\n",
    "    )\n",
    "    if len(faces) == 0: return None\n",
    "    return max(faces, key=lambda r: r[2]*r[3])\n",
    "\n",
    "def expand_box(x, y, w, h, W, H, fx=EXPAND_X, fy=EXPAND_Y):\n",
    "    cx, cy = x + w//2, y + h//2\n",
    "    w2, h2 = int(w*(1+2*fx)), int(h*(1+2*fy))\n",
    "    # bias upward to include forehead\n",
    "    x2 = max(0, cx - w2//2)\n",
    "    y2 = max(0, cy - int(0.55*h2))\n",
    "    x2e, y2e = min(W, x2 + w2), min(H, y2 + h2)\n",
    "    return x2, y2, max(1, x2e-x2), max(1, y2e-y2)\n",
    "\n",
    "def crop_face(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    box = detect_largest(gray)\n",
    "    if box is None: return None\n",
    "    H, W = frame.shape[:2]\n",
    "    x,y,w,h = expand_box(*box, W=W, H=H)\n",
    "    face = frame[y:y+h, x:x+w]\n",
    "    if face.size == 0: return None\n",
    "    return cv2.resize(face, (IMSIZE, IMSIZE), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# -------------- Quality gate --------------\n",
    "def quality_ok(face_bgr):\n",
    "    if face_bgr is None or face_bgr.size == 0:\n",
    "        return False, \"empty\"\n",
    "    gray = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    if min(gray.shape[:2]) < MIN_FACE_PX:\n",
    "        return False, \"too small\"\n",
    "    blur = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    if blur < MIN_BLUR: return False, f\"blurry({blur:.1f})\"\n",
    "    m = float(gray.mean())\n",
    "    if m < MIN_BRIGHT: return False, f\"dark({m:.1f})\"\n",
    "    if m > MAX_BRIGHT: return False, f\"bright({m:.1f})\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "# -------------- Capture helpers --------------\n",
    "def draw_info(frame, msg, color=(0,255,0)):\n",
    "    cv2.putText(frame, msg, (10, 24), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)\n",
    "\n",
    "def capture_avg_embedding(n_frames=5, window=\"Camera\"):\n",
    "    cap = cv2.VideoCapture(CAM_INDEX, cv2.CAP_DSHOW)\n",
    "    if not cap.isOpened():\n",
    "        print(\"[ERR] cannot open camera\"); return None\n",
    "    print(f\"Press SPACE to capture {n_frames} frames, ESC to quit.\")\n",
    "    vecs = []\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: print(\"[ERR] grab failed\"); break\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        show = frame.copy()\n",
    "        draw_info(show, f\"SPACE: capture {n_frames} | ESC: exit\")\n",
    "        cv2.imshow(window, show)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:  # ESC\n",
    "            print(\"Exit.\"); vecs=[]; break\n",
    "        if k == 32:  # SPACE\n",
    "            taken = 0\n",
    "            while taken < n_frames:\n",
    "                ok2, f2 = cap.read()\n",
    "                if not ok2: break\n",
    "                f2 = cv2.flip(f2, 1)\n",
    "                face = crop_face(f2)\n",
    "                okq, why = quality_ok(face)\n",
    "                if not okq:\n",
    "                    draw_info(f2, f\"Skip: {why}\", (0,0,255)); cv2.imshow(window, f2); cv2.waitKey(1)\n",
    "                    continue\n",
    "                vecs.append(embed_bgr(face))\n",
    "                taken += 1\n",
    "                draw_info(f2, f\"Captured {taken}/{n_frames}\", (0,255,0))\n",
    "                cv2.imshow(window, f2); cv2.waitKey(1)\n",
    "            break\n",
    "    cap.release(); cv2.destroyAllWindows()\n",
    "    if not vecs: return None\n",
    "    v = np.mean(np.stack(vecs,0), axis=0)\n",
    "    v = v / (np.linalg.norm(v) + 1e-9)\n",
    "    return v.astype(np.float32)\n",
    "\n",
    "# -------------- Verify two captures --------------\n",
    "def verify_two_captures(tau=DEFAULT_TAU):\n",
    "    e1 = capture_avg_embedding(ENROLL_AVG_N, window=\"Enroll\")\n",
    "    if e1 is None: return None, None, None\n",
    "    e2 = capture_avg_embedding(PROBE_AVG_N, window=\"Probe\")\n",
    "    if e2 is None: return None, None, None\n",
    "    sim = cosine(e1, e2)\n",
    "    ok  = sim >= tau\n",
    "    print(f\"[VERIFY] cosine={sim:.4f} | tau={tau:.2f} → {'MATCH ✅' if ok else 'NOT MATCH ❌'}\")\n",
    "    return e1, e2, (sim, ok)\n",
    "\n",
    "# -------------- Main --------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"[INFO] Starting two-capture verify (Haar + averaging + quality gate).\")\n",
    "    e1, e2, res = verify_two_captures(tau=DEFAULT_TAU)\n",
    "    if res is not None:\n",
    "        sim, ok = res\n",
    "        print(f\"\\nResult → cosine={sim:.4f} vs tau={DEFAULT_TAU:.2f} :: {'MATCH ✅' if ok else 'NOT MATCH ❌'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
