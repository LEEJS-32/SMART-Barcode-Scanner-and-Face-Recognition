{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a08953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1: Core model + preprocessing (ORB-friendly) + verify (affine RANSAC) + inline display ===\n",
    "import cv2, os, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Haar cascade ----------\n",
    "def load_face_cascade():\n",
    "    candidates = [\n",
    "        \"models/haarcascade_frontalface_default.xml\",\n",
    "        os.path.join(os.getcwd(), \"haarcascade_frontalface_default.xml\"),\n",
    "        os.path.join(cv2.data.haarcascades, \"haarcascade_frontalface_default.xml\"),\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            c = cv2.CascadeClassifier(p)\n",
    "            if not c.empty():\n",
    "                return c\n",
    "    raise IOError(\"Haar cascade not found. Put 'haarcascade_frontalface_default.xml' in ./models/ or project root.\")\n",
    "\n",
    "_FACE_CASCADE = load_face_cascade()\n",
    "\n",
    "def detect_largest_face_bgr(bgr_img, scaleFactor=1.1, minNeighbors=5):\n",
    "    if bgr_img is None: return None, None\n",
    "    gray = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = _FACE_CASCADE.detectMultiScale(gray, scaleFactor, minNeighbors)\n",
    "    if len(faces) == 0: return None, None\n",
    "    (x, y, w, h) = max(faces, key=lambda r: r[2]*r[3])\n",
    "    return (x, y, w, h), bgr_img[y:y+h, x:x+w]\n",
    "\n",
    "# ---------- Preprocessing (ORB-friendly: no blur; preserve corners/edges) ----------\n",
    "def preprocess_face(bgr_face, out_size=(160,160)):\n",
    "    gray = cv2.cvtColor(bgr_face, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    eq = clahe.apply(gray)\n",
    "    # IMPORTANT: do NOT blur for ORB (keeps corner strength)\n",
    "    resized = cv2.resize(eq, out_size, interpolation=cv2.INTER_AREA)\n",
    "    return resized  # grayscale\n",
    "\n",
    "# ---------- ORB features (stronger config) ----------\n",
    "def extract_orb_features(gray_img, nfeatures=2000):\n",
    "    # More features, more robust descriptors; lower FAST threshold to pick up low-contrast corners\n",
    "    orb = cv2.ORB_create(\n",
    "        nfeatures=nfeatures,\n",
    "        scaleFactor=1.2,\n",
    "        nlevels=8,\n",
    "        edgeThreshold=15,\n",
    "        firstLevel=0,\n",
    "        WTA_K=4,\n",
    "        scoreType=cv2.ORB_HARRIS_SCORE,\n",
    "        patchSize=31,\n",
    "        fastThreshold=7\n",
    "    )\n",
    "    kps, des = orb.detectAndCompute(gray_img, None)\n",
    "    return kps, des\n",
    "\n",
    "# ---------- Matching & scoring (ratio test + affine RANSAC inliers) ----------\n",
    "def match_and_score(des1, des2, ratio=0.78, require_model=True, model=\"affine\"):\n",
    "    \"\"\"\n",
    "    Returns (score, good_matches, inlier_mask)\n",
    "      - score = #inliers if a geometric model is estimated; else len(good_matches)\n",
    "      - model: \"affine\" (preferred for faces) or \"homography\"\n",
    "    \"\"\"\n",
    "    if des1 is None or des2 is None or len(des1) < 2 or len(des2) < 2:\n",
    "        return 0, [], None\n",
    "\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "    knn = bf.knnMatch(des1, des2, k=2)\n",
    "    good = [m for m, n in knn if m.distance < ratio * n.distance]\n",
    "\n",
    "    inlier_mask = None\n",
    "    if require_model and len(good) >= 6:\n",
    "        src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        if model == \"affine\":\n",
    "            # Similarity/affine model is more realistic for faces than a full homography\n",
    "            M, mask = cv2.estimateAffinePartial2D(\n",
    "                src_pts, dst_pts, method=cv2.RANSAC, ransacReprojThreshold=4.0, maxIters=2000, confidence=0.99\n",
    "            )\n",
    "        else:\n",
    "            H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "        if mask is not None:\n",
    "            inlier_mask = mask.ravel().tolist()\n",
    "            return int(np.sum(mask)), good, inlier_mask\n",
    "\n",
    "    return len(good), good, inlier_mask\n",
    "\n",
    "# ---------- Verify wrapper (affine inliers preferred; fallback to good-match count) ----------\n",
    "def verify_faces(ref_gray, cap_gray, threshold_inliers=10, threshold_goods=18, force_model=\"affine\"):\n",
    "    \"\"\"\n",
    "    Returns: decision (bool), score (int), dbg (dict)\n",
    "      - decision uses inlier count if a model was found; otherwise good-match count.\n",
    "      - thresholds are set to be reasonable for your images; tune if needed.\n",
    "    \"\"\"\n",
    "    global kp1, kp2\n",
    "    kp1, des1 = extract_orb_features(ref_gray)\n",
    "    kp2, des2 = extract_orb_features(cap_gray)\n",
    "\n",
    "    score, good_matches, inlier_mask = match_and_score(\n",
    "        des1, des2, ratio=0.78, require_model=True, model=force_model\n",
    "    )\n",
    "\n",
    "    used_metric = \"inliers\" if inlier_mask is not None else \"good_matches\"\n",
    "    threshold = threshold_inliers if used_metric == \"inliers\" else threshold_goods\n",
    "    decision = score >= threshold\n",
    "\n",
    "    dbg = {\n",
    "        \"used_metric\": used_metric,\n",
    "        \"threshold\": threshold,\n",
    "        \"score\": score,\n",
    "        \"num_kp_ref\": 0 if kp1 is None else len(kp1),\n",
    "        \"num_kp_cap\": 0 if kp2 is None else len(kp2),\n",
    "        \"good_matches\": len(good_matches),\n",
    "        \"inlier_mask\": inlier_mask,\n",
    "        \"good_matches_list\": good_matches\n",
    "    }\n",
    "    return decision, score, dbg\n",
    "\n",
    "# ---------- Draw matches (keeps your inline visualization flow) ----------\n",
    "def draw_matches(ref_gray, cap_gray, matches, inlier_mask=None):\n",
    "    flags = cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    if inlier_mask is not None:\n",
    "        inlier_matches = [m for m, keep in zip(matches, inlier_mask) if keep]\n",
    "        return cv2.drawMatches(ref_gray, kp1, cap_gray, kp2, inlier_matches, None, flags=flags)\n",
    "    return cv2.drawMatches(ref_gray, kp1, cap_gray, kp2, matches, None, flags=flags)\n",
    "\n",
    "# ---------- Inline display helpers (Notebook output) ----------\n",
    "def _to_rgb(img):\n",
    "    if img.ndim == 2:      # gray\n",
    "        return img\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def show_pair(title, left_img, right_img, left_label=\"Left\", right_label=\"Right\", figsize=(9,4)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.suptitle(title, fontsize=16, fontweight=\"bold\")\n",
    "    plt.subplot(1,2,1); plt.imshow(_to_rgb(left_img), cmap=None if left_img.ndim==3 else \"gray\")\n",
    "    plt.title(left_label); plt.axis('off')\n",
    "    plt.subplot(1,2,2); plt.imshow(_to_rgb(right_img), cmap=None if right_img.ndim==3 else \"gray\")\n",
    "    plt.title(right_label); plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_single(title, img, figsize=(6,6)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title, fontsize=14, fontweight=\"bold\")\n",
    "    plt.imshow(_to_rgb(img), cmap=None if img.ndim==3 else \"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c31911a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Live camera capture (returns largest face crop in BGR) ===\n",
    "import cv2\n",
    "\n",
    "def capture_and_verify_loop(ref_image=None):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    captured_face = None\n",
    "\n",
    "    print(\"Live camera: SPACE=capture, Q/ESC=quit\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: camera read failed.\")\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = _FACE_CASCADE.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            for (x, y, w, h) in faces:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "                cv2.putText(frame, \"Face Detected\", (x, y-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Live Camera Feed\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == ord(' '):  # capture\n",
    "            if len(faces) == 0:\n",
    "                print(\"No face detected. Try again.\")\n",
    "                continue\n",
    "            x, y, w, h = max(faces, key=lambda r: r[2]*r[3])\n",
    "            captured_face = frame[y:y+h, x:x+w]\n",
    "            break\n",
    "        elif key in (ord('q'), 27):  # 'q' or ESC\n",
    "            print(\"Exit.\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return captured_face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc13b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: With camera — inline compares + printed summary ===\n",
    "import cv2\n",
    "\n",
    "# 1) Load reference original (change to your file)\n",
    "ref_path = r\"data/faces/Phua Hong Yip/jia ling.jpg\"\n",
    "ref_bgr = cv2.imread(ref_path)\n",
    "if ref_bgr is None:\n",
    "    raise FileNotFoundError(f\"Cannot read reference image: {ref_path}\")\n",
    "\n",
    "# 2) Detect reference face\n",
    "_, ref_face_bgr = detect_largest_face_bgr(ref_bgr)\n",
    "if ref_face_bgr is None:\n",
    "    raise RuntimeError(\"No face detected in reference image.\")\n",
    "\n",
    "# 3) Capture live face\n",
    "cap_face_bgr = capture_and_verify_loop()\n",
    "if cap_face_bgr is None:\n",
    "    raise RuntimeError(\"Capture canceled or no face captured.\")\n",
    "\n",
    "# 4) Preprocess both\n",
    "ref_face_pp = preprocess_face(ref_face_bgr)\n",
    "cap_face_pp = preprocess_face(cap_face_bgr)\n",
    "\n",
    "# 5) Show inline compares (no windows)\n",
    "show_pair(\"Compare 1: Reference — Original vs Preprocessed\",\n",
    "          ref_face_bgr, ref_face_pp,\n",
    "          \"Reference (Original)\", \"Reference (Preprocessed)\")\n",
    "\n",
    "show_pair(\"Compare 2: Capture — Original vs Preprocessed\",\n",
    "          cap_face_bgr, cap_face_pp,\n",
    "          \"Capture (Original)\", \"Capture (Preprocessed)\")\n",
    "\n",
    "show_pair(\"Compare 3: Preprocessed — Reference vs Capture\",\n",
    "          ref_face_pp, cap_face_pp,\n",
    "          \"Ref (Preprocessed)\", \"Cap (Preprocessed)\")\n",
    "\n",
    "# 6) Verify and show matches (preprocessed vs preprocessed)\n",
    "decision, score, dbg = verify_faces(ref_face_pp, cap_face_pp)\n",
    "match_vis = draw_matches(ref_face_pp, cap_face_pp, dbg['good_matches_list'], dbg['inlier_mask'])\n",
    "show_single(\"Compare 4: Verification — ORB Matches / Inliers (Preprocessed Ref vs Cap)\", match_vis)\n",
    "\n",
    "# 7) Print concise summary\n",
    "print(\"=========== FACE VERIFY SUMMARY ===========\")\n",
    "print(f\"Metric used      : {dbg['used_metric']}\")\n",
    "print(f\"Threshold        : {dbg['threshold']}\")\n",
    "print(f\"Score            : {dbg['score']}\")\n",
    "print(f\"Keypoints (ref)  : {dbg['num_kp_ref']}\")\n",
    "print(f\"Keypoints (cap)  : {dbg['num_kp_cap']}\")\n",
    "print(f\"Good matches     : {dbg['good_matches']}\")\n",
    "print(f\"Decision         : {'MATCH ✅' if decision else 'NOT MATCH ❌'}\")\n",
    "print(\"===========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4a0568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL]  identities: 31\n",
      "[TEST] identities: 31\n",
      "[VAL] pairs: 1362 (pos_cap=20, neg_cap=25)\n",
      "[VAL] usable: 1152, skipped (no face): 210\n",
      "[VAL] τ*=6.500 | EER≈0.3818 | ACC=0.6120 | F1=0.5947 | ROC_AUC=0.671571\n",
      "[TEST] pairs: 3360\n",
      "[TEST] usable: 2750, skipped (no face): 610\n",
      "[TEST] ACC=0.6564 | Precision=0.8530 | Recall=0.6774 | F1=0.7551 | ROC_AUC=0.692222\n",
      "\n",
      "[TEST] Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Different       0.33      0.58      0.42       599\n",
      "        Same       0.85      0.68      0.76      2151\n",
      "\n",
      "    accuracy                           0.66      2750\n",
      "   macro avg       0.59      0.63      0.59      2750\n",
      "weighted avg       0.74      0.66      0.68      2750\n",
      "\n",
      "[TEST] Confusion Matrix:\n",
      " [[ 348  251]\n",
      " [ 694 1457]]\n",
      "\n",
      "[IDENTIFY] Gallery/train per-ID=1 → Top-1 ACC=0.2306 (83/360)\n",
      "\n",
      "Saved:\n",
      " - orb_eval_out/orb_threshold_and_metrics.json\n",
      " - orb_eval_out/scores_val.npy, labels_val.npy\n",
      " - orb_eval_out/scores_test.npy, labels_test.npy\n",
      " - orb_eval_out/identification_report.json\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: ORB Verification/Identification Evaluation on Your Dataset ===\n",
    "import os, json, random, itertools, numpy as np, cv2\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "    HAVE_SK = True\n",
    "except Exception:\n",
    "    HAVE_SK = False\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATA_ROOT      = r\"C:\\Users\\jians\\Documents\\GitHub\\SMART-Barcode-Scanner-and-Face-Recognition\\data\\data_proc\"     # <- change if needed\n",
    "VAL_DIRNAME    = \"val\"             # expects DATA_ROOT/val/<ID>/*.jpg\n",
    "TEST_DIRNAME   = \"test\"            # expects DATA_ROOT/test/<ID>/*.jpg\n",
    "TRAIN_DIRNAME  = \"train\"           # for identification gallery\n",
    "ALLOWED_EXT    = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "\n",
    "# Pair sampling caps (set to None for 'all')\n",
    "POS_PER_ID_VAL   = 20\n",
    "NEG_PER_ID_VAL   = 25\n",
    "POS_PER_ID_TEST  = None\n",
    "NEG_PER_ID_TEST  = 25\n",
    "\n",
    "RANSAC_MODEL     = \"affine\"  # \"affine\" (recommended) or \"homography\"\n",
    "\n",
    "OUT_DIR          = \"orb_eval_out\"\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def list_id_images(split_dir):\n",
    "    \"\"\"\n",
    "    Returns dict: {id_name: [image_paths...]}\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    p = Path(split_dir)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Split not found: {split_dir}\")\n",
    "    for sub in sorted(p.iterdir()):\n",
    "        if sub.is_dir():\n",
    "            imgs = [str(x) for x in sorted(sub.rglob(\"*\")) if x.suffix.lower() in ALLOWED_EXT]\n",
    "            if imgs:\n",
    "                out[sub.name] = imgs\n",
    "    return out\n",
    "\n",
    "def load_face_gray_160(img_path):\n",
    "    \"\"\"\n",
    "    Loads BGR, detects largest face, preprocesses to your ORB-friendly grayscale 160x160.\n",
    "    Returns gray image or None if detection fails.\n",
    "    \"\"\"\n",
    "    bgr = cv2.imread(img_path)\n",
    "    if bgr is None:\n",
    "        return None\n",
    "    _, face_bgr = detect_largest_face_bgr(bgr)\n",
    "    if face_bgr is None:\n",
    "        return None\n",
    "    return preprocess_face(face_bgr, out_size=(160,160))  # uses your function\n",
    "\n",
    "def orb_pair_score(gray1, gray2, model=RANSAC_MODEL):\n",
    "    \"\"\"\n",
    "    Returns (score:int, used_metric:str) where score = #inliers if model found,\n",
    "    otherwise #good_matches. Uses your ORB + match pipeline.\n",
    "    \"\"\"\n",
    "    global kp1, kp2  # required by your match_and_score() for drawing/mapping\n",
    "    kp1, des1 = extract_orb_features(gray1)\n",
    "    kp2, des2 = extract_orb_features(gray2)\n",
    "    score, good_matches, inlier_mask = match_and_score(\n",
    "        des1, des2, ratio=0.78, require_model=True, model=model\n",
    "    )\n",
    "    used = \"inliers\" if inlier_mask is not None else \"good_matches\"\n",
    "    return int(score), used\n",
    "\n",
    "def make_pairs(id2imgs, pos_cap=None, neg_cap=None):\n",
    "    \"\"\"\n",
    "    Build (imgA, imgB, label) pairs.\n",
    "     - Positive: within each ID.\n",
    "     - Negative: across different IDs.\n",
    "    Caps are per-ID (so it scales on big datasets).\n",
    "    \"\"\"\n",
    "    rng = random.Random(1337)\n",
    "    pairs = []\n",
    "\n",
    "    # Positives\n",
    "    for _id, imgs in id2imgs.items():\n",
    "        if len(imgs) < 2: \n",
    "            continue\n",
    "        combos = list(itertools.combinations(imgs, 2))\n",
    "        rng.shuffle(combos)\n",
    "        if pos_cap is not None:\n",
    "            combos = combos[:pos_cap]\n",
    "        pairs.extend([(a, b, 1) for (a, b) in combos])\n",
    "\n",
    "    # Negatives\n",
    "    ids = list(id2imgs.keys())\n",
    "    for i, id_a in enumerate(ids):\n",
    "        imgs_a = id2imgs[id_a]\n",
    "        others = ids[:i] + ids[i+1:]\n",
    "        if not others or not imgs_a:\n",
    "            continue\n",
    "        # choose some 'b' identities randomly\n",
    "        rng.shuffle(others)\n",
    "        neg_samples = []\n",
    "        for id_b in others:\n",
    "            imgs_b = id2imgs[id_b]\n",
    "            if not imgs_b: \n",
    "                continue\n",
    "            # one random cross-pair per other-id to diversify\n",
    "            a = rng.choice(imgs_a)\n",
    "            b = rng.choice(imgs_b)\n",
    "            neg_samples.append((a, b, 0))\n",
    "        rng.shuffle(neg_samples)\n",
    "        if neg_cap is not None:\n",
    "            neg_samples = neg_samples[:neg_cap]\n",
    "        pairs.extend(neg_samples)\n",
    "\n",
    "    rng.shuffle(pairs)\n",
    "    return pairs\n",
    "\n",
    "def score_pairs(pairs):\n",
    "    \"\"\"\n",
    "    Returns (scores:list, labels:list, bad:list_of_indices)\n",
    "    Skips pairs where face detection fails; records their indices in 'bad'.\n",
    "    \"\"\"\n",
    "    scores, labels, bad = [], [], []\n",
    "    for idx, (pa, pb, y) in enumerate(pairs):\n",
    "        ga = load_face_gray_160(pa)\n",
    "        gb = load_face_gray_160(pb)\n",
    "        if ga is None or gb is None:\n",
    "            bad.append(idx)\n",
    "            continue\n",
    "        s, _ = orb_pair_score(ga, gb)\n",
    "        scores.append(s); labels.append(y)\n",
    "    return np.array(scores, dtype=float), np.array(labels, dtype=int), bad\n",
    "\n",
    "def find_tau_star(scores, labels):\n",
    "    \"\"\"\n",
    "    Find threshold τ* that minimizes |FNR - FPR| (approx. EER point).\n",
    "    Returns tau_star, EER, stats dict.\n",
    "    \"\"\"\n",
    "    if len(scores) == 0:\n",
    "        return 0.0, 1.0, {\"note\": \"empty scores\"}\n",
    "    uniq = np.unique(scores)\n",
    "    # Consider midpoints between sorted unique scores to be safer\n",
    "    cuts = np.concatenate(([uniq[0]-1], (uniq[:-1]+uniq[1:])/2.0, [uniq[-1]+1]))\n",
    "    best_tau, best_gap, best_eer = None, 1.0, 1.0\n",
    "    for tau in cuts:\n",
    "        preds = (scores >= tau).astype(int)\n",
    "        TP = ((preds==1) & (labels==1)).sum()\n",
    "        TN = ((preds==0) & (labels==0)).sum()\n",
    "        FP = ((preds==1) & (labels==0)).sum()\n",
    "        FN = ((preds==0) & (labels==1)).sum()\n",
    "        P  = max(1, (labels==1).sum())\n",
    "        N  = max(1, (labels==0).sum())\n",
    "        TPR = TP / P\n",
    "        FPR = FP / N\n",
    "        FNR = 1 - TPR\n",
    "        gap = abs(FNR - FPR)\n",
    "        eer = (FNR + FPR)/2.0\n",
    "        if gap < best_gap:\n",
    "            best_gap, best_tau, best_eer = gap, float(tau), float(eer)\n",
    "    stats = {\"gap\": best_gap}\n",
    "    return best_tau, best_eer, stats\n",
    "\n",
    "def bin_metrics(scores, labels, tau):\n",
    "    preds = (scores >= tau).astype(int)\n",
    "    TP = ((preds==1) & (labels==1)).sum()\n",
    "    TN = ((preds==0) & (labels==0)).sum()\n",
    "    FP = ((preds==1) & (labels==0)).sum()\n",
    "    FN = ((preds==0) & (labels==1)).sum()\n",
    "\n",
    "    acc = (TP + TN) / max(1, len(labels))\n",
    "    prec = TP / max(1, (TP + FP))\n",
    "    rec  = TP / max(1, (TP + FN))\n",
    "    f1   = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
    "    out = {\n",
    "        \"ACC\": round(float(acc), 6),\n",
    "        \"Precision\": round(float(prec), 6),\n",
    "        \"Recall\": round(float(rec), 6),\n",
    "        \"F1\": round(float(f1), 6),\n",
    "        \"TP\": int(TP), \"TN\": int(TN), \"FP\": int(FP), \"FN\": int(FN),\n",
    "        \"Support_Pos\": int((labels==1).sum()),\n",
    "        \"Support_Neg\": int((labels==0).sum())\n",
    "    }\n",
    "    if HAVE_SK:\n",
    "        try:\n",
    "            out[\"ROC_AUC\"] = round(float(roc_auc_score(labels, scores)), 6)\n",
    "        except Exception:\n",
    "            out[\"ROC_AUC\"] = None\n",
    "    else:\n",
    "        out[\"ROC_AUC\"] = None\n",
    "    return out, preds\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "# ---------- Identification (optional) ----------\n",
    "def build_gallery(id2imgs_train, per_id=1):\n",
    "    \"\"\"\n",
    "    Returns dict {id_name: [preprocessed_gray_images]} using up to per_id images per identity.\n",
    "    \"\"\"\n",
    "    gal = {}\n",
    "    for _id, paths in id2imgs_train.items():\n",
    "        picked = paths[:per_id]\n",
    "        grays = []\n",
    "        for p in picked:\n",
    "            g = load_face_gray_160(p)\n",
    "            if g is not None:\n",
    "                grays.append(g)\n",
    "        if grays:\n",
    "            gal[_id] = grays\n",
    "    return gal\n",
    "\n",
    "def identify_top1(gallery, id2imgs_test):\n",
    "    \"\"\"\n",
    "    For each test image, compare to gallery of each ID, take max score; choose ID with max.\n",
    "    Returns top1 accuracy and per-ID stats.\n",
    "    \"\"\"\n",
    "    total, correct = 0, 0\n",
    "    per_id = defaultdict(lambda: {\"n\":0, \"correct\":0})\n",
    "\n",
    "    ids = list(id2imgs_test.keys())\n",
    "    for true_id in ids:\n",
    "        for p in id2imgs_test[true_id]:\n",
    "            g = load_face_gray_160(p)\n",
    "            if g is None:\n",
    "                continue\n",
    "            best_id, best_score = None, -1\n",
    "            for gid, gimgs in gallery.items():\n",
    "                # score against all gallery imgs for that ID, take max\n",
    "                scores = []\n",
    "                for gg in gimgs:\n",
    "                    s, _ = orb_pair_score(g, gg)\n",
    "                    scores.append(s)\n",
    "                if scores:\n",
    "                    smax = max(scores)\n",
    "                    if smax > best_score:\n",
    "                        best_score, best_id = smax, gid\n",
    "            if best_id is None:\n",
    "                continue\n",
    "            total += 1\n",
    "            if best_id == true_id:\n",
    "                correct += 1\n",
    "                per_id[true_id][\"correct\"] += 1\n",
    "            per_id[true_id][\"n\"] += 1\n",
    "\n",
    "    acc = 0.0 if total==0 else correct/total\n",
    "    return acc, total, correct, per_id\n",
    "\n",
    "# ---------- Run full evaluation ----------\n",
    "def evaluate_and_report(\n",
    "    data_root=DATA_ROOT,\n",
    "    val_dirname=VAL_DIRNAME,\n",
    "    test_dirname=TEST_DIRNAME,\n",
    "    train_dirname=TRAIN_DIRNAME,\n",
    "    pos_per_id_val=POS_PER_ID_VAL,\n",
    "    neg_per_id_val=NEG_PER_ID_VAL,\n",
    "    pos_per_id_test=POS_PER_ID_TEST,\n",
    "    neg_per_id_test=NEG_PER_ID_TEST,\n",
    "    gallery_per_id=1,\n",
    "    do_identification=True\n",
    "):\n",
    "    # 1) Index splits\n",
    "    val_ids  = list_id_images(os.path.join(data_root, val_dirname))\n",
    "    test_ids = list_id_images(os.path.join(data_root, test_dirname))\n",
    "    print(f\"[VAL]  identities: {len(val_ids)}\")\n",
    "    print(f\"[TEST] identities: {len(test_ids)}\")\n",
    "\n",
    "    # 2) Build validation pairs → choose tau*\n",
    "    pairs_val = make_pairs(val_ids, pos_cap=pos_per_id_val, neg_cap=neg_per_id_val)\n",
    "    print(f\"[VAL] pairs: {len(pairs_val)} (pos_cap={pos_per_id_val}, neg_cap={neg_per_id_val})\")\n",
    "    scores_val, labels_val, bad_val = score_pairs(pairs_val)\n",
    "    print(f\"[VAL] usable: {len(scores_val)}, skipped (no face): {len(bad_val)}\")\n",
    "    tau_star, eer, tau_meta = find_tau_star(scores_val, labels_val)\n",
    "    val_metrics, _ = bin_metrics(scores_val, labels_val, tau_star)\n",
    "    print(f\"[VAL] τ*={tau_star:.3f} | EER≈{eer:.4f} | ACC={val_metrics['ACC']:.4f} | F1={val_metrics['F1']:.4f} | ROC_AUC={val_metrics['ROC_AUC']}\")\n",
    "\n",
    "    # 3) Test pairs → fixed tau*\n",
    "    pairs_test = make_pairs(test_ids, pos_cap=pos_per_id_test, neg_cap=neg_per_id_test)\n",
    "    print(f\"[TEST] pairs: {len(pairs_test)}\")\n",
    "    scores_test, labels_test, bad_test = score_pairs(pairs_test)\n",
    "    print(f\"[TEST] usable: {len(scores_test)}, skipped (no face): {len(bad_test)}\")\n",
    "    test_metrics, preds_test = bin_metrics(scores_test, labels_test, tau_star)\n",
    "    print(f\"[TEST] ACC={test_metrics['ACC']:.4f} | Precision={test_metrics['Precision']:.4f} | Recall={test_metrics['Recall']:.4f} | F1={test_metrics['F1']:.4f} | ROC_AUC={test_metrics['ROC_AUC']}\")\n",
    "    if HAVE_SK:\n",
    "        print(\"\\n[TEST] Classification report:\")\n",
    "        try:\n",
    "            print(classification_report(labels_test, preds_test, target_names=[\"Different\",\"Same\"]))\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            cm = confusion_matrix(labels_test, preds_test)\n",
    "            print(\"[TEST] Confusion Matrix:\\n\", cm)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 4) Save artifacts\n",
    "    save_json({\n",
    "        \"tau_star\": tau_star,\n",
    "        \"eer\": eer,\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"notes\": {\"tau_search\": tau_meta}\n",
    "    }, os.path.join(OUT_DIR, \"orb_threshold_and_metrics.json\"))\n",
    "\n",
    "    np.save(os.path.join(OUT_DIR, \"scores_val.npy\"),  scores_val)\n",
    "    np.save(os.path.join(OUT_DIR, \"labels_val.npy\"),  labels_val)\n",
    "    np.save(os.path.join(OUT_DIR, \"scores_test.npy\"), scores_test)\n",
    "    np.save(os.path.join(OUT_DIR, \"labels_test.npy\"), labels_test)\n",
    "\n",
    "    # 5) Optional identification (Top-1)\n",
    "    id_report = None\n",
    "    if do_identification:\n",
    "        train_ids = list_id_images(os.path.join(data_root, train_dirname))\n",
    "        gallery = build_gallery(train_ids, per_id=gallery_per_id)\n",
    "        id_acc, total, correct, per_id = identify_top1(gallery, test_ids)\n",
    "        id_report = {\n",
    "            \"top1_acc\": round(float(id_acc), 6),\n",
    "            \"total_probes\": int(total),\n",
    "            \"correct\": int(correct),\n",
    "            \"per_id\": {k: {\"n\": v[\"n\"], \"correct\": v[\"correct\"]} for k, v in per_id.items()}\n",
    "        }\n",
    "        save_json(id_report, os.path.join(OUT_DIR, \"identification_report.json\"))\n",
    "        print(f\"\\n[IDENTIFY] Gallery/train per-ID={gallery_per_id} → Top-1 ACC={id_acc:.4f} ({correct}/{total})\")\n",
    "\n",
    "    return tau_star, eer, val_metrics, test_metrics, id_report\n",
    "\n",
    "# ---- Run (edit paths/caps if needed) ----\n",
    "tau_star, eer, val_metrics, test_metrics, id_report = evaluate_and_report(\n",
    "    data_root=DATA_ROOT,\n",
    "    val_dirname=VAL_DIRNAME,\n",
    "    test_dirname=TEST_DIRNAME,\n",
    "    train_dirname=TRAIN_DIRNAME,\n",
    "    pos_per_id_val=POS_PER_ID_VAL,\n",
    "    neg_per_id_val=NEG_PER_ID_VAL,\n",
    "    pos_per_id_test=POS_PER_ID_TEST,\n",
    "    neg_per_id_test=NEG_PER_ID_TEST,\n",
    "    gallery_per_id=1,          # 1 ref image per ID in train for identification\n",
    "    do_identification=True\n",
    ")\n",
    "print(\"\\nSaved:\")\n",
    "print(f\" - {OUT_DIR}/orb_threshold_and_metrics.json\")\n",
    "print(f\" - {OUT_DIR}/scores_val.npy, labels_val.npy\")\n",
    "print(f\" - {OUT_DIR}/scores_test.npy, labels_test.npy\")\n",
    "if id_report is not None:\n",
    "    print(f\" - {OUT_DIR}/identification_report.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
